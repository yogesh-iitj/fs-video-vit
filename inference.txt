import os
import argparse
import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import cv2
from tqdm import tqdm
from pathlib import Path
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from matplotlib.colors import hsv_to_rgb

from model import FewShotVideoObjectDetection

def generate_distinct_colors(n):
    """Generate n distinct colors"""
    colors = []
    for i in range(n):
        hue = i / n
        saturation = 0.9
        value = 0.9
        rgb = hsv_to_rgb((hue, saturation, value))
        rgb = tuple(int(255 * x) for x in rgb)
        colors.append(rgb)
    return colors

def load_images_from_folder(folder_path):
    """Load all images from a folder"""
    images = []
    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    
    for file in sorted(os.listdir(folder_path)):
        file_path = os.path.join(folder_path, file)
        if os.path.isfile(file_path) and any(file.lower().endswith(ext) for ext in valid_extensions):
            try:
                img = Image.open(file_path).convert('RGB')
                images.append((file, img))
            except Exception as e:
                print(f"Error loading image {file_path}: {e}")
    
    return images

def preprocess_image(image, target_size=(640, 640)):
    """Preprocess image for model input"""
    transform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    return transform(image)

def draw_detections(image, detections, class_names, class_colors, threshold=0.5):
    """Draw detections on an image"""
    draw = ImageDraw.Draw(image)
    
    # Try to load a font
    try:
        font = ImageFont.truetype("arial.ttf", 20)
    except IOError:
        font = ImageFont.load_default()
    
    # Draw each detection
    for box, score, label in zip(detections['boxes'], detections['scores'], detections['labels']):
        if score < threshold:
            continue
        
        # Get color and label name
        color = class_colors[label]
        label_name = class_names.get(label, str(label))
        
        # Draw box
        x1, y1, x2, y2 = map(int, box)
        draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
        
        # Draw label
        text = f"{label_name}: {score:.2f}"
        text_size = draw.textsize(text, font=font)
        draw.rectangle([x1, y1, x1 + text_size[0], y1 + text_size[1]], fill=color)
        draw.text((x1, y1), text, fill=(255, 255, 255), font=font)
    
    return image

def process_video(
    video_path,
    support_folder,
    output_path,
    model,
    device,
    img_size=640,
    confidence_threshold=0.5,
    class_names=None
):
    """Process a video using few-shot video object detection"""
    # Load support images
    support_images = {}
    
    # Each subfolder in support_folder is a class
    for class_folder in os.listdir(support_folder):
        class_path = os.path.join(support_folder, class_folder)
        if os.path.isdir(class_path):
            # Use folder name as class ID
            class_id = class_folder
            
            # Load images from this folder
            class_images = load_images_from_folder(class_path)
            
            if not class_images:
                print(f"Warning: No images found in {class_path}")
                continue
            
            # Preprocess images
            processed_images = [preprocess_image(img) for _, img in class_images]
            support_images[class_id] = torch.stack(processed_images).to(device)
    
    if not support_images:
        raise ValueError("No support images found. Please check support folder structure.")
    
    # Generate colors for classes
    class_ids = list(support_images.keys())
    colors = generate_distinct_colors(len(class_ids))
    class_colors = {cls_id: color for cls_id, color in zip(class_ids, colors)}
    
    # If class_names not provided, use class IDs
    if class_names is None:
        class_names = {cls_id: cls_id for cls_id in class_ids}
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise ValueError(f"Could not open video {video_path}")
    
    # Get video properties
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Initialize video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # Process frames
    prev_features = None
    prev_mask = None
    
    with tqdm(total=num_frames, desc="Processing video") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_pil = Image.fromarray(frame_rgb)
            
            # Preprocess frame
            frame_tensor = preprocess_image(frame_pil).unsqueeze(0).to(device)
            
            # Extract features
            with torch.no_grad():
                # Extract frame features
                frame_features = model.extract_features(frame_tensor)
                
                # Process frame with temporal context
                frame_output = model.forward_frame(
                    frame_features=frame_features,
                    class_prototypes={cls_id: imgs.mean(dim=0) for cls_id, imgs in support_images.items()},
                    prev_object_features=prev_features,
                    prev_object_mask=prev_mask
                )
                
                
                # Get image size
                image_size = (height, width)
                
                # Get post-processed detections
                detections = model.postprocess_detections(
                    frame_output,
                    image_size,
                    confidence_threshold=confidence_threshold
                )
            
            # Draw detections on frame
            frame_with_dets = draw_detections(
                frame_pil,
                detections,
                class_names,
                class_colors,
                threshold=confidence_threshold
            )
            
            # Convert back to BGR for video writing
            frame_with_dets_np = np.array(frame_with_dets)
            frame_with_dets_bgr = cv2.cvtColor(frame_with_dets_np, cv2.COLOR_RGB2BGR)
            
            # Write to video
            out.write(frame_with_dets_bgr)
            
            # Update progress bar
            pbar.update(1)
    
    # Release resources
    cap.release()
    out.release()
    
    print(f"Processed video saved to {output_path}")

def main():
    parser = argparse.ArgumentParser(description="Few-Shot Video Object Detection Inference")
    
    # Input/output arguments
    parser.add_argument('--video_path', type=str, required=True,
                        help="Path to input video file")
    parser.add_argument('--support_folder', type=str, required=True,
                        help="Path to support images folder (one subfolder per class)")
    parser.add_argument('--output_path', type=str, default="output_video.mp4",
                        help="Path to save output video")
    
    # Model arguments
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help="Path to model checkpoint")
    parser.add_argument('--pretrained_model', type=str, default="google/owlv2-large-patch16",
                        help="Pretrained model name or path")
    parser.add_argument('--conf_threshold', type=float, default=0.5,
                        help="Confidence threshold for detections")
    parser.add_argument('--temporal_threshold', type=float, default=0.94,
                        help="Threshold for temporal propagation")
    
    # Other arguments
    parser.add_argument('--img_size', type=int, default=640,
                        help="Image size for processing")
    parser.add_argument('--gpu', type=int, default=0,
                        help="GPU ID, -1 for CPU")
    parser.add_argument('--class_names_file', type=str, default=None,
                        help="JSON file mapping class IDs to readable names")
    
    args = parser.parse_args()
    
    # Set device
    device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() and args.gpu >= 0 else "cpu")
    print(f"Using device: {device}")
    
    # Load class names if provided
    class_names = None
    if args.class_names_file and os.path.exists(args.class_names_file):
        import json
        with open(args.class_names_file, 'r') as f:
            class_names = json.load(f)
    
    # Create model
    model = FewShotVideoObjectDetection(
        pretrained_model_name=args.pretrained_model,
        confidence_threshold=args.conf_threshold,
        temporal_confidence_threshold=args.temporal_threshold
    )
    
    # Load checkpoint
    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    model.eval()
    
    print(f"Loaded checkpoint from {args.checkpoint_path}")
    
    # Process video
    process_video(
        video_path=args.video_path,
        support_folder=args.support_folder,
        output_path=args.output_path,
        model=model,
        device=device,
        img_size=args.img_size,
        confidence_threshold=args.conf_threshold,
        class_names=class_names
    )

if __name__ == "__main__":
    main()